{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMH0wNtyTLFCIzZ9YOxvScM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yujia1104/fe/blob/main/NLP_Assignment2_223040118.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 1: LLMs as a knowledgeable doctor**"
      ],
      "metadata": {
        "id": "38qXu4FtAbWC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pharmacist licensure exam is a cornerstone in the pharmacy profession, ensuring that candidates possess the requisite knowledge and skills for safe and effective practice. Its significance lies not only in validating credentials but also in safeguarding public health, enabling professional recognition, and ensuring adherence to legal and regulatory standards.\n",
        "\n",
        "Advanced models like ChatGPT have significant potential in exam preparation, boasting an extensive knowledge base and the capability to provide in-depth explanations and clarify complex concepts. However, despite the prowess of such large models, if prompts are not designed appropriately, the information retrieved might be inaccurate or incomplete, potentially hindering success in the pharmacist exam."
      ],
      "metadata": {
        "id": "3WbAkCovAjHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install langchain-openai\n",
        "!pip install langchain-deepseek\n",
        "!pip install retrying\n",
        "!pip install langchain_core\n",
        "!pip install tqdm\n",
        "!pip install jsonlines"
      ],
      "metadata": {
        "collapsed": true,
        "id": "2O0KhWOZB-Eh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_deepseek import ChatDeepSeek\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "import random\n",
        "import json\n",
        "from retrying import retry\n",
        "import requests\n",
        "\n",
        "os.environ[\"DEEPSEEK_API_KEY\"] = \"sk-d5afb39115974c6f840eaea469828fda\"          #deepseek\n",
        "os.environ[\"DEEPSEEK_BASE_URL\"] = \"https://api.deepseek.com/v1\"\n",
        "\n",
        "deepseek_chat = ChatDeepSeek(model=\"deepseek-chat\", temperature=1)"
      ],
      "metadata": {
        "id": "G8IalI68CDQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are an AI assistant, please answer user's question.\"),\n",
        "        (\"user\", \"{input}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "model = ChatDeepSeek(model=\"deepseek-chat\")"
      ],
      "metadata": {
        "id": "RGzJ2vbZEgCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | model | StrOutputParser()\n",
        "\n",
        "response = chain.invoke({\"input\": \"Hello\"})"
      ],
      "metadata": {
        "id": "7c2iAsekEgyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "JYS0VrOTEjYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://NLP-course-cuhksz.github.io/Assignments/Assignment1/task1/data/1.exam.json"
      ],
      "metadata": {
        "id": "vp5rXduVAma0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcaf955b-ce8b-4402-89fa-f70d41e96f3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-25 13:06:08--  https://nlp-course-cuhksz.github.io/Assignments/Assignment1/task1/data/1.exam.json\n",
            "Resolving nlp-course-cuhksz.github.io (nlp-course-cuhksz.github.io)... 185.199.108.153, 185.199.111.153, 185.199.110.153, ...\n",
            "Connecting to nlp-course-cuhksz.github.io (nlp-course-cuhksz.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 86227 (84K) [application/json]\n",
            "Saving to: ‘1.exam.json.1’\n",
            "\n",
            "1.exam.json.1       100%[===================>]  84.21K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-03-25 13:06:09 (834 KB/s) - ‘1.exam.json.1’ saved [86227/86227]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open('1.exam.json') as f:\n",
        "  data = json.load(f)"
      ],
      "metadata": {
        "id": "dZ5VXp2zAnLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[0]"
      ],
      "metadata": {
        "id": "AxEq8f1aBoco",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dc3b05c-2aaa-48af-d3cc-d6e125882892"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': '27. 根据国家药品监督管理局，公安部，国家卫⽣健康委员会的有关规定，⼜服固体制剂每剂量单位含羟考酮碱不超过5毫克，且不含其他⿇醉药品，精神药品或者药品类易制毒化学品的复⽅制剂列⼊（）。',\n",
              " 'option': {'A': '含⿇醉药品复⽅制剂的管理',\n",
              "  'B': '第⼆类精神药品管理',\n",
              "  'C': '第⼀类精神药品管理',\n",
              "  'D': '医疗⽤毒性药品管理',\n",
              "  'E': ''},\n",
              " 'analysis': '⼜服固体制剂每剂量单位含羟考酮碱不超过5毫克，且不含其他⿇醉药品、精神药品或药品类易制毒化学品的复⽅制剂列⼊第⼆类精神药品管理。',\n",
              " 'answer': 'B',\n",
              " 'question_type': '最佳选择题',\n",
              " 'source': '2021年执业药师职业资格考试《药事管理与法规》'}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "your_prompt = \"\"\"请回答下面的多选题，请直接正确答案选项，不要输出其他内容。\n",
        "{question}\n",
        "{options}\"\"\"\n",
        "\n",
        "def get_query(da):\n",
        "  da['options'] = '\\n'.join([f\"{k}:{v}\" for k,v in da['option'].items()])\n",
        "  return your_prompt.format_map(da)\n",
        "\n",
        "get_query(data[0])"
      ],
      "metadata": {
        "id": "s83squcQBqmG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "7090bb01-1364-4092-bbe6-8d47a6f75115"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'请回答下面的多选题，请直接正确答案选项，不要输出其他内容。\\n27. 根据国家药品监督管理局，公安部，国家卫⽣健康委员会的有关规定，⼜服固体制剂每剂量单位含羟考酮碱不超过5毫克，且不含其他⿇醉药品，精神药品或者药品类易制毒化学品的复⽅制剂列⼊（）。\\nA:含⿇醉药品复⽅制剂的管理\\nB:第⼆类精神药品管理\\nC:第⼀类精神药品管理\\nD:医疗⽤毒性药品管理\\nE:'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke(get_query(data[0]))"
      ],
      "metadata": {
        "id": "h1CI-TaBBt__",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7e59b878-5c99-4c8c-b7e5-ccf0aaedbf1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'B:第⼆类精神药品管理'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Default"
      ],
      "metadata": {
        "id": "iQPaa7UQG7ZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 计算做题准确率\n",
        "your_prompt = \"\"\"Please answer the multiple choice questions below, please direct the correct answer option and do not output anything else.\n",
        "{question}\n",
        "{options}\"\"\"\n",
        "\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "def get_ans(ans):\n",
        "    match = re.findall(r'.*?([A-E]+(?:[、, ]+[A-E]+)*)', ans)\n",
        "    if match:\n",
        "        last_match = match[-1]\n",
        "        return ''.join(re.split(r'[、, ，]+', last_match))\n",
        "    return ''\n",
        "\n",
        "correct_num = 0\n",
        "total_num = 0\n",
        "for da in tqdm(data[:100]):\n",
        "  da['deepseek_ans'] =  chain.invoke(get_query(da))\n",
        "  if get_ans(da['deepseek_ans']) == da['answer']:\n",
        "    correct_num += 1\n",
        "  total_num += 1\n",
        "print(f\"模型准确率: {correct_num/total_num:.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cpy38QbdGmi3",
        "outputId": "9cfaa1ad-0b2f-4787-f3c3-2d985de00c51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [08:35<00:00,  5.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "模型准确率: 78.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# few_shot_prompt\n",
        "your_prompt = \"\"\"Here are some examples of how to answer professional multiple-choice questions:\n",
        "\n",
        "Example 1:\n",
        "Question: 根据以下规定，哪项属于第一类精神药品管理？\n",
        "Options: A. 氯胺酮 B. 哌替啶 C. 吗啡 D. 芬太尼\n",
        "Answer: C\n",
        "\n",
        "Example 2:\n",
        "Question: 以下关于执业药师的职责描述，哪一项是正确的？\n",
        "Options: A. 只能在药店工作 B. 仅限处方药指导 C. 提供用药咨询 D. 不可进行药品调配\n",
        "Answer: C\n",
        "\n",
        "Now, please answer the following multiple-choice question:\n",
        "Question: {question}\n",
        "Options:\n",
        "{options}\n",
        "\n",
        "Please answer using the format: Answer: [OPTION LETTER]\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "def get_ans(ans):\n",
        "    match = re.findall(r'.*?([A-E]+(?:[、, ]+[A-E]+)*)', ans)\n",
        "    if match:\n",
        "        last_match = match[-1]\n",
        "        return ''.join(re.split(r'[、, ，]+', last_match))\n",
        "    return ''\n",
        "\n",
        "correct_num = 0\n",
        "total_num = 0\n",
        "for da in tqdm(data[:100]):\n",
        "  da['deepseek_ans'] =  chain.invoke(get_query(da))\n",
        "  if get_ans(da['deepseek_ans']) == da['answer']:\n",
        "    correct_num += 1\n",
        "  total_num += 1\n",
        "print(f\"模型准确率: {correct_num/total_num:.2%}\")"
      ],
      "metadata": {
        "id": "fMGLvLIFG5mK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e80b0f0-89e4-4204-b79a-72198a5bcfcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [08:40<00:00,  5.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "模型准确率: 80.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# chain_of_thought_prompt\n",
        "your_prompt = \"\"\"Let’s break down the following medical/pharmaceutical regulatory question step by step.\n",
        "\n",
        "Question: {question}\n",
        "Options:\n",
        "{options}\n",
        "\n",
        "Step 1: Identify the key regulatory terms or numerical thresholds in the question.\n",
        "Step 2: Recall the relevant classification rules or policies (e.g., regarding drug control categories).\n",
        "Step 3: Eliminate incorrect options based on mismatch with the criteria.\n",
        "Step 4: Choose the most accurate answer based on the remaining valid choice.\n",
        "\n",
        "Provide your reasoning first, then give the final answer as: Answer: [OPTION LETTER]\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "def get_ans(ans):\n",
        "    # First, look for multiple answers or formatted choices\n",
        "    match = re.findall(r'[A-E]+(?:[、, ]+[A-E]+)*', ans)\n",
        "    if match:\n",
        "        last_match = match[-1]\n",
        "        # Clean up the extracted answer, remove any unwanted characters\n",
        "        return ''.join(re.split(r'[、, ，]+', last_match))\n",
        "    return ans.strip()\n",
        "\n",
        "correct_num = 0\n",
        "total_num = 0\n",
        "for da in tqdm(data[:100]):\n",
        "  da['deepseek_ans'] =  chain.invoke(get_query(da))\n",
        "  if get_ans(da['deepseek_ans']) == da['answer']:\n",
        "    correct_num += 1\n",
        "  total_num += 1\n",
        "print(f\"模型准确率: {correct_num/total_num:.2%}\")"
      ],
      "metadata": {
        "id": "i2jeJ8DCHQ2G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "674f42fa-5cac-4236-e197-ae5ad729d4e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [39:31<00:00, 23.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "模型准确率: 87.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# self_consistency_prompt\n",
        "your_prompt = \"\"\"You are answering a multiple-choice regulatory question. You will approach it from three different reasoning paths, then decide which answer appears most consistently.\n",
        "\n",
        "Question: {question}\n",
        "Options:\n",
        "{options}\n",
        "\n",
        "Answer the question three times using varied reasoning or assumptions. Then, identify the most frequently occurring answer.\n",
        "\n",
        "Output only the final consistent answer in the format: Answer: [OPTION LETTER]\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "def get_ans(ans):\n",
        "    match = re.findall(r'.*?([A-E]+(?:[、, ]+[A-E]+)*)', ans)\n",
        "    if match:\n",
        "        last_match = match[-1]\n",
        "        return ''.join(re.split(r'[、, ，]+', last_match))\n",
        "    return ''\n",
        "\n",
        "correct_num = 0\n",
        "total_num = 0\n",
        "for da in tqdm(data[:100]):\n",
        "  da['deepseek_ans'] =  chain.invoke(get_query(da))\n",
        "  if get_ans(da['deepseek_ans']) == da['answer']:\n",
        "    correct_num += 1\n",
        "  total_num += 1\n",
        "print(f\"模型准确率: {correct_num/total_num:.2%}\")"
      ],
      "metadata": {
        "id": "0mJxcTesG1Wv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ffde303-7ebf-4519-f42a-73c126c2d73d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [31:38<00:00, 18.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "模型准确率: 81.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">You need to optimize the prompt to improve the performance (accuracy) of large language models (LLMs).</font>"
      ],
      "metadata": {
        "id": "PdBtYzl4B1p6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agent-based"
      ],
      "metadata": {
        "id": "9QGXHoCJg2Qj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from langchain_deepseek import ChatDeepSeek\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# 环境配置\n",
        "os.environ[\"DEEPSEEK_API_KEY\"] = \"sk-d5afb39115974c6f840eaea469828fda\"  # deepseek\n",
        "os.environ[\"DEEPSEEK_BASE_URL\"] = \"https://api.deepseek.com/v1\"\n",
        "\n",
        "# 初始化模型和提示模板\n",
        "# 确保传入模型参数\n",
        "deepseek_chat = ChatDeepSeek(model=\"deepseek-chat\", model_kwargs={\"temperature\": 1.0})  # 确保提供模型名称\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are an AI assistant, please answer user's question.\"),\n",
        "        (\"user\", \"{input}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 设置模型处理链\n",
        "model = deepseek_chat\n",
        "chain = prompt | model | StrOutputParser()\n",
        "\n",
        "# 下载题目数据\n",
        "!wget https://NLP-course-cuhksz.github.io/Assignments/Assignment1/task1/data/1.exam.json\n",
        "\n",
        "# 加载数据\n",
        "with open('1.exam.json') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# 定义模板格式\n",
        "your_prompt = \"\"\"请回答下面的多选题，请直接正确答案选项，不要输出其他内容。\n",
        "{question}\n",
        "{options}\"\"\"\n",
        "\n",
        "def get_query(da):\n",
        "    da['options'] = '\\n'.join([f\"{k}:{v}\" for k,v in da['option'].items()])\n",
        "    return your_prompt.format_map(da)\n",
        "\n",
        "# 提取答案函数\n",
        "def get_ans(ans):\n",
        "    match = re.findall(r'.*?([A-E]+(?:[、, ]+[A-E]+)*)', ans)\n",
        "    if match:\n",
        "        last_match = match[-1]\n",
        "        return ''.join(re.split(r'[、, ，]+', last_match))\n",
        "    return ''\n",
        "\n",
        "class QuestionAnswerAgent:\n",
        "    def __init__(self, model_chain):\n",
        "        self.model_chain = model_chain\n",
        "        self.correct_num = 0\n",
        "        self.total_num = 0\n",
        "\n",
        "    def ask_question(self, question_data):\n",
        "        query = get_query(question_data)\n",
        "        return self.model_chain.invoke(query)\n",
        "\n",
        "    def evaluate_answer(self, predicted_answer, correct_answer):\n",
        "        if get_ans(predicted_answer) == correct_answer:\n",
        "            self.correct_num += 1\n",
        "        self.total_num += 1\n",
        "\n",
        "    def get_accuracy(self):\n",
        "        return self.correct_num / self.total_num if self.total_num > 0 else 0\n",
        "\n",
        "# 创建并使用 Agent\n",
        "agent = QuestionAnswerAgent(model_chain=chain)\n",
        "\n",
        "# 评估模型准确率\n",
        "for da in tqdm(data[:100]):\n",
        "    deepseek_answer = agent.ask_question(da)\n",
        "    agent.evaluate_answer(deepseek_answer, da['answer'])\n",
        "\n",
        "# 输出模型准确率\n",
        "print(f\"模型准确率: {agent.get_accuracy():.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HouS9nLs0rQM",
        "outputId": "2ec0fa5a-23ac-49a9-cb19-38677376e0b7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py:3473: UserWarning: Parameters {'temperature'} should be specified explicitly. Instead they were passed in as part of `model_kwargs` parameter.\n",
            "  if (await self.run_code(code, result,  async_=asy)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-25 15:01:40--  https://nlp-course-cuhksz.github.io/Assignments/Assignment1/task1/data/1.exam.json\n",
            "Resolving nlp-course-cuhksz.github.io (nlp-course-cuhksz.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to nlp-course-cuhksz.github.io (nlp-course-cuhksz.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 86227 (84K) [application/json]\n",
            "Saving to: ‘1.exam.json.5’\n",
            "\n",
            "1.exam.json.5       100%[===================>]  84.21K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-03-25 15:01:40 (847 KB/s) - ‘1.exam.json.5’ saved [86227/86227]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [08:10<00:00,  4.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "模型准确率: 71.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}